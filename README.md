# FlexiMart Data Architecture Project

**Student Name:** Thatikonda Sai Lakshmi Reeshma
**Student ID:** bitsom_ba_25071594
**Email:** reeshma6345@gmail.com
**Date:** 07-01-2026

## Project Overview

This project focuses on designing and implementing a complete data architecture solution for FlexiMart. 
It includes building a relational database with an ETL pipeline, analyzing flexible product data using MongoDB, 
and creating a data warehouse with a star schema for analytical reporting. 
The project demonstrates how operational data can be transformed into meaningful insights for business decision-making.


## Repository Structure
studentID-fleximart-data-architecture/
│
├── README.md                           # Root documentation
├── .gitignore                          # Ignore unnecessary files
│
├── data/                               # Input data files (provided)
│   ├── customers_raw.csv
│   ├── products_raw.csv
│   └── sales_raw.csv
│   ├── customers_clean.csv
│   ├── products_clean.csv
│   └── sales_clean.csv
│
├── part1-database-etl/
│   ├── README.md                       # Part 1 overview
│   ├── etl_pipeline.py
│   ├── schema_documentation.md
│   ├── business_queries.sql
│   ├── data_quality_report.txt         # Generated by ETL script
│   ├── etl_log.txt                     # Generated when ETL script is run
│   └── requirements.txt                # Python dependencies
│
├── part2-nosql/
│   ├── README.md                       # Part 2 overview
│   ├── nosql_analysis.md
│   ├── mongodb_operations.js
│   └── products_catalog.json
│
└── part3-datawarehouse/
    ├── README.md                       # Part 3 overview
    ├── star_schema_design.md
    ├── warehouse_schema.sql
    ├── warehouse_data.sql
    └── analytics_queries.sql



## Technologies Used

- **Python 3.x** (ETL pipeline and data processing)
- **MySQL 8.0** (Relational database and data warehouse)
- **MongoDB 6.0** (NoSQL product catalog)
- **mysql-connector-python**
- **SQL** (Analytics and reporting)

## Setup Instructions


### Database Setup (MySQL)

You can create and load the databases either **using command-line scripts** (recommended for consistency) **or manually using MySQL Workbench**.

#### Option 1: Using Command Line (Recommended)


# Create databases
mysql -u root -p -e "CREATE DATABASE fleximart;"
mysql -u root -p -e "CREATE DATABASE fleximart_dw;"

# Run Part 1 - ETL Pipeline
python part1-database-etl/etl_pipeline.py

# Run Part 1 - Business Queries
mysql -u root -p fleximart < part1-database-etl/business_queries.sql

# Run Part 3 - Data Warehouse
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_schema.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_data.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/analytics_queries.sql


# Using MySQL Workbench (Manual Method)

- Open MySQL Workbench

- Create databases fleximart and fleximart_dw

- Open each .sql file and execute it manually

- Run the ETL pipeline using:
    python part1-database-etl/etl_pipeline.py
    
### MongoDB Setup

MongoDB collections can also be created either using scripts or manually in MongoDB Compass.

  mongosh < part2-nosql/mongodb_operations.js


------Alternative:-------

- Open MongoDB Compass

- Create the database and collection manually

Import products_catalog.json directly into the collection

## Key Learnings

Through this project, I learned how to design normalized relational schemas and build an end-to-end ETL pipeline with proper data cleaning and validation. 
I also understood when and why NoSQL databases like MongoDB are useful for handling flexible and nested data structures. 
Additionally, I gained practical experience in dimensional modeling, star schema design, and writing OLAP queries for business analytics.

## Challenges Faced

# Handling foreign key errors in fact tables
While loading data into the fact_sales table, foreign key constraint errors occurred because some dimension records were not available at the time of insertion. Since fact tables depend on dimension tables, inserting sales data before loading date, product, or customer information caused failures.
Solution: I ensured that all dimension tables (dim_date, dim_product, and dim_customer) were populated first. I also carefully verified that every foreign key used in the fact table existed in the corresponding dimension tables before inserting the data. This step-by-step loading order helped avoid foreign key violations.

# Managing different date formats and missing values during ETL
The raw data contained dates in multiple formats and some records had missing customer or product information. This caused inconsistencies during data transformation and loading.
Solution: I implemented date normalization logic to convert all dates into a standard format before processing. For missing values, I applied explicit data imputation rules and documented the reasons for these corrections using comments and logs. This approach ensured consistent data quality and made the ETL process transparent and easy to understand.
