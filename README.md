# FlexiMart Data Architecture Project

**Student Name:** Thatikonda Sai Lakshmi Reeshma
**Student ID:** bitsom_ba_25071594
**Email:** reeshma6345@gmail.com
**Date:** 07-01-2026

## Project Overview

This project focuses on designing and implementing a complete data architecture solution for FlexiMart. 
It includes building a relational database with an ETL pipeline, analyzing flexible product data using MongoDB, 
and creating a data warehouse with a star schema for analytical reporting. 
The project demonstrates how operational data can be transformed into meaningful insights for business decision-making.


## Repository Structure
studentID-fleximart-data-architecture/
│
├── README.md                           # Root documentation
├── .gitignore                          # Ignore unnecessary files
│
├── data/                               # Input data files (provided)
│   ├── customers_raw.csv
│   ├── products_raw.csv
│   └── sales_raw.csv
│   ├── customers_clean.csv
│   ├── products_clean.csv
│   └── sales_clean.csv
│
├── part1-database-etl/
│   ├── README.md                       # Part 1 overview
│   ├── etl_pipeline.py
│   ├── schema_documentation.md
│   ├── business_queries.sql
│   ├── data_quality_report.txt         # Generated by ETL script
│   ├── etl_log.txt                     # Generated when ETL script is run
│   └── requirements.txt                # Python dependencies
│
├── part2-nosql/
│   ├── README.md                       # Part 2 overview
│   ├── nosql_analysis.md
│   ├── mongodb_operations.js
│   └── products_catalog.json
│
└── part3-datawarehouse/
    ├── README.md                       # Part 3 overview
    ├── star_schema_design.md
    ├── warehouse_schema.sql
    ├── warehouse_data.sql
    └── analytics_queries.sql



## Technologies Used

- **Python 3.x** (ETL pipeline and data processing)
- **MySQL 8.0** (Relational database and data warehouse)
- **MongoDB 6.0** (NoSQL product catalog)
- **mysql-connector-python**
- **SQL** (Analytics and reporting)

## Setup Instructions


### Database Setup (MySQL)

You can create and load the databases either **using command-line scripts** (recommended for consistency) **or manually using MySQL Workbench**.

#### Option 1: Using Command Line (Recommended)


# Create databases
mysql -u root -p -e "CREATE DATABASE fleximart;"
mysql -u root -p -e "CREATE DATABASE fleximart_dw;"

# Run Part 1 - ETL Pipeline
python part1-database-etl/etl_pipeline.py

# Run Part 1 - Business Queries
mysql -u root -p fleximart < part1-database-etl/business_queries.sql

# Run Part 3 - Data Warehouse
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_schema.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_data.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/analytics_queries.sql


# Using MySQL Workbench (Manual Method)

- Open MySQL Workbench

- Create databases fleximart and fleximart_dw

- Open each .sql file and execute it manually

- Run the ETL pipeline using:
    python part1-database-etl/etl_pipeline.py
    
### MongoDB Setup

MongoDB collections can also be created either using scripts or manually in MongoDB Compass.

  mongosh < part2-nosql/mongodb_operations.js


------Alternative:

- Open MongoDB Compass

- Create the database and collection manually

Import products_catalog.json directly into the collection

## Key Learnings

Through this project, I learned how to design normalized relational schemas and build an end-to-end ETL pipeline with proper data cleaning and validation. 
I also understood when and why NoSQL databases like MongoDB are useful for handling flexible and nested data structures. 
Additionally, I gained practical experience in dimensional modeling, star schema design, and writing OLAP queries for business analytics.

## Challenges Faced

1. Handling foreign key errors in fact tables
Solution: Ensured that dimension tables were populated first and validated all foreign key references before inserting fact data.

2. Managing different date formats and missing values during ETL
Solution: Implemented date normalization logic and explicit data imputation with clear documentation and comments.
